{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bilm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f9960ccb5bdf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspatial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistance\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbilm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBatcher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBidirectionalLanguageModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mopenpyxl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'bilm'"
     ]
    }
   ],
   "source": [
    "#Use with python/3.6.0 and tensorflow/1.10\n",
    "\n",
    "import math\n",
    "import sys\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import random\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn import metrics\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from glove import Corpus, Glove\n",
    "#from bert_serving.client import BertClient\n",
    "#bc = BertClient()\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.spatial.distance as ds\n",
    "from bilm import Batcher, BidirectionalLanguageModel, weight_layers\n",
    "import openpyxl\n",
    " \n",
    "\n",
    "\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, Dropout\n",
    "\n",
    "from tensorflow.python.keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.python.keras.constraints import maxnorm\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import make_scorer, r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "lib_path = os.path.abspath(os.path.join('..', 'lib'))\n",
    "sys.path.append(lib_path)\n",
    "\n",
    "ds = \"mimic\"\n",
    "data_path = \"/Users/emily/Documents/SequentialPhenotypePredictor-master/Data/mimic_seq/\"\n",
    "#data_path = \"/users/emily/Documents/mimic_seq/\"\n",
    "window=10\n",
    "size=100\n",
    "decay=5\n",
    "skipgram=1\n",
    "norm=False\n",
    "random.seed(1)\n",
    "\n",
    "# list for files\n",
    "train_files = []\n",
    "valid_files = []\n",
    "full_data_files = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(7):\n",
    "    train_files.append(data_path + 'test_'+str(i))\n",
    "for i in range(7,10):\n",
    "    valid_files.append(data_path + 'test_'+str(i))\n",
    "for i in range(10):\n",
    "    full_data_files.append(data_path + 'test_'+str(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag_totals = defaultdict(lambda: 0)\n",
    "diag_totals2 = defaultdict(lambda: 0)\n",
    "diag_joined = defaultdict(lambda: 0)\n",
    "diag_joined2 = defaultdict(lambda:0)\n",
    "test = defaultdict(lambda:0)\n",
    "sentences = []\n",
    "final_dx_train = []\n",
    "disease_prev = []\n",
    "list_duplicates = []\n",
    "diag = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/anaconda3/envs/[py3.6]/lib/python3.6/site-packages/bilm/model.py:384: calling squeeze (from tensorflow.python.ops.array_ops) with squeeze_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the `axis` argument instead\n",
      "USING SKIP CONNECTIONS\n"
     ]
    }
   ],
   "source": [
    "# Location of pretrained LM.  Here we use the test fixtures.\n",
    "# Location of pretrained LM.  Here we use the test fixtures.\n",
    "#datadir = '/users/emily/Documents/swp'\n",
    "#vocab_file = '/users/emily/Documents/swp/vocab.txt'\n",
    "#options_file = '/users/emily/Documents/swp/checkpoint/options.json'\n",
    "#weight_file = '/users/emily/Documents/swp/swp_weights.hdf5'\n",
    "datadir = '/home/egetzen/swp'\n",
    "vocab_file = '/home/egetzen/swp/vocab.txt'\n",
    "options_file = '/home/egetzen/swp/checkpoint/options.json'\n",
    "weight_file = '/home/egetzen/swp/swp_weights.hdf5'\n",
    "batcher = Batcher(vocab_file, 50)\n",
    "context_character_ids = tf.placeholder('int32', shape=(None, None, 50))\n",
    "bilm = BidirectionalLanguageModel(options_file, weight_file)\n",
    "context_embeddings_op = bilm(context_character_ids)\n",
    "elmo_context_input = weight_layers('input', context_embeddings_op, l2_coef=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = -1\n",
    "for i in train_files:\n",
    "    with open(i) as f:\n",
    "        for s in f:\n",
    "            count = count + 1\n",
    "            diag.append(s.split(\"|\")[0].split(\",\")) \n",
    "\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "flat_diag = list(itertools.chain(*diag))\n",
    "events = np.unique(flat_diag)\n",
    "events = np.ndarray.tolist(events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "passed = ['d_041']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = -1\n",
    "for i in train_files:\n",
    "    with open(i) as f:\n",
    "        for s in f:\n",
    "            count = count + 1\n",
    "            sentences.append(s.split(\"|\")[2].split(\" \") +\n",
    "                             s.split(\"|\")[3].replace(\"\\n\", \"\").split(\" \"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "flat_sentences = list(itertools.chain(*sentences))\n",
    "events = np.unique(flat_sentences)\n",
    "events = np.ndarray.tolist(events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-81ec9119b8df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    232\u001b[0m                             elmo_context_input_ = sess.run(\n\u001b[1;32m    233\u001b[0m                                 \u001b[0melmo_context_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weighted_op'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                                 \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mcontext_character_ids\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcontext_ids\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m                             )\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/[py3.6]/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/[py3.6]/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1100\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/[py3.6]/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1272\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1273\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/[py3.6]/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1276\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/[py3.6]/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1261\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1263\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/[py3.6]/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pred = 'lasso'\n",
    "\n",
    "if pred == 'lasso':\n",
    "    path_auc = '/users/emily/Documents/BERT_lasso_AUC'\n",
    "    path_f1 = '/users/emily/Documents/BERT_lasso_F1'\n",
    "    \n",
    "\n",
    "if pred == 'DL':\n",
    "    path_auc = '/users/emily/Documents/BERT_DL_AUC'\n",
    "    path_f1 = '/users/emily/Documents/BERT_DL_F1'\n",
    "\n",
    "AUC = []\n",
    "F1 = []\n",
    "\n",
    "for q in range(len(passed)):  \n",
    "    AUC_hold=[]\n",
    "    F1_hold = []\n",
    "\n",
    "    for w in range(3):\n",
    "        \n",
    "\n",
    "        def prep_data(data_files):\n",
    "            disease_prev = []\n",
    "            omit1 = []\n",
    "            sentences = []\n",
    "            count = -1\n",
    "            disease_prev = []\n",
    "            omit1 = []\n",
    "            count = -1\n",
    "            for i in data_files:\n",
    "                with open(i) as f:\n",
    "                    for s in f:\n",
    "                        count = count+1\n",
    "                        age = float(s.split(\"|\")[1].split(\" \")[3].replace(\",\",r\"\"))\n",
    "                        if age <= 5:\n",
    "                            omit1.append(1)\n",
    "                        else:\n",
    "                            omit1.append(0)\n",
    "                            \n",
    "                        prev_diags = [e for e in s.split(\"|\")[2].split(\" \") if e.startswith(\"d_\")]\n",
    "                        if passed[q] in prev_diags: # if disease exit in the previous diagnosis\n",
    "                            disease_prev.append(1)\n",
    "                        else:\n",
    "                            disease_prev.append(0)\n",
    "                        sentences.append(s.split(\"|\")[2].split(\" \") +\n",
    "                                             s.split(\"|\")[3].replace(\"\\n\", \"\").split(\" \"))\n",
    "                        \n",
    "\n",
    "            return sentences, omit1, disease_prev\n",
    "        sentences, omit1, disease_prev = prep_data(train_files)\n",
    "        \n",
    "\n",
    "\n",
    "        ## Within each sentence, split medical record into individual visits  \n",
    "        def split_sentences(sentences):\n",
    "            newsents = []\n",
    "            for count in range(len(sentences)):\n",
    "                partials = []    \n",
    "                z = 0\n",
    "                for i in range(len(sentences[count])-1):        \n",
    "                    i = i+1  \n",
    "                    if (sentences[count])[i-1].startswith(\"d_\"):             \n",
    "                        if not sentences[count][i].startswith(\"d_\") or (i == len(sentences[count])):\n",
    "                            part = sentences[count][z:i]\n",
    "                            partials.append(part)\n",
    "                            z = i\n",
    "\n",
    "                    if (sentences[count])[i-1].startswith(\"s\"):\n",
    "                        if not sentences[count][i].startswith(\"d_\"): \n",
    "                            if not sentences[count][i].startswith(\"s\") or (i == len(sentences[count])):\n",
    "                                part = sentences[count][z:i]\n",
    "                                partials.append(part)\n",
    "                                z = i\n",
    "\n",
    "                    if (sentences[count])[i-1].startswith(\"c\"):\n",
    "                        if not sentences[count][i].startswith(\"d_\"):\n",
    "                            if not sentences[count][i].startswith(\"s\"): \n",
    "                                if not sentences[count][i].startswith(\"c\") or (i == len(sentences[count])):\n",
    "                                    part = sentences[count][z:i]\n",
    "                                    partials.append(part)\n",
    "                                    z = i\n",
    "\n",
    "                newsents.append(partials)\n",
    "            return newsents\n",
    "        newsents = split_sentences(sentences)\n",
    "\n",
    "        ## Within each sentence, split medical record into individual visits  \n",
    "        def split_sentences2(sentences):\n",
    "            newsents = []\n",
    "            for count in range(len(sentences)):\n",
    "                partials = []    \n",
    "                z = 0\n",
    "                for i in range(len(sentences[count])-1):        \n",
    "                    i = i+1  \n",
    "                    if (sentences[count])[i-1].startswith(\"d_\"):             \n",
    "                        if not sentences[count][i].startswith(\"d_\") or (i == len(sentences[count])-1):\n",
    "                            part = sentences[count][z:i]\n",
    "                            partials.append(part)\n",
    "                            z = i\n",
    "\n",
    "                    if (sentences[count])[i-1].startswith(\"s\"):\n",
    "                        if not sentences[count][i].startswith(\"d_\"): \n",
    "                            if not sentences[count][i].startswith(\"s\") or (i == len(sentences[count])-1):\n",
    "                                part = sentences[count][z:i]\n",
    "                                partials.append(part)\n",
    "                                z = i\n",
    "\n",
    "                    if (sentences[count])[i-1].startswith(\"c\"):\n",
    "                        if not sentences[count][i].startswith(\"d_\"):\n",
    "                            if not sentences[count][i].startswith(\"s\"): \n",
    "                                if not sentences[count][i].startswith(\"c\") or (i == len(sentences[count])-1):\n",
    "                                    part = sentences[count][z:i]\n",
    "                                    partials.append(part)\n",
    "                                    z = i\n",
    "\n",
    "                newsents.append(partials)\n",
    "            return newsents\n",
    "        newsents2 = split_sentences2(sentences)\n",
    "\n",
    "        #target_loc = []\n",
    "        def adjust_all(newsents):\n",
    "            # Identify the visit in the history where the target diagnosis occurs, if it does                                    \n",
    "            target_loc = []\n",
    "            ct1 = 0\n",
    "            ct2 = 0\n",
    "            for i in range(len(sentences)):\n",
    "                ct = 0\n",
    "                if passed[q] in list(np.concatenate(newsents[i])):\n",
    "                    for j in range(len(newsents[i])):\n",
    "                        if ct == 0 and passed[q] in (newsents[i])[j]:\n",
    "                            target_loc.append(j)\n",
    "                            ct = ct + 1\n",
    "                            ct1 = ct1+1\n",
    "                else:\n",
    "                    target_loc.append(100)\n",
    "                    ct2 = ct2+1\n",
    "            return target_loc\n",
    "        target_loc = adjust_all(newsents2)\n",
    "\n",
    "        #data refers to train or test \n",
    "        def adjust_exclusions(sentences, newsents,omit):                    \n",
    "            # create a vector indicating which patients are positive for target diagnosis\n",
    "\n",
    "            disease_data = []\n",
    "            for count in range(len(sentences)):  \n",
    "                if passed[q] in sentences[count]:\n",
    "                    disease_data.append(1)\n",
    "                else:\n",
    "                    disease_data.append(0)\n",
    "\n",
    "\n",
    "            #exclude patients with target diagnosis present in first visit\n",
    "            exc = []                \n",
    "            for ii in range(len(sentences)): \n",
    "                if target_loc[ii]==0:\n",
    "                    exc.append(1)\n",
    "                else:\n",
    "                    exc.append(0)\n",
    "\n",
    "            #Update vector containing patients to exclude\n",
    "            for count in range(len(sentences)):\n",
    "                if exc[count]==1:\n",
    "                    omit[count] = 1\n",
    "\n",
    "            return disease_data, omit\n",
    "\n",
    "        disease_train, omit1 = adjust_exclusions(sentences,newsents,omit1)\n",
    "\n",
    "\n",
    "    #Convert events in history to patient vector\n",
    "\n",
    "\n",
    "        def patient_vec(data_files, newsents, disease_prev, target,omit):\n",
    "            count = -1\n",
    "            patient_seq_all_disease = []\n",
    "            exclude = []\n",
    " \n",
    "            for i in data_files: \n",
    "                with open(i) as f:\n",
    "                    for line in f:\n",
    "                        count = count + 1\n",
    "                        patient_seq = []\n",
    "                        #feed_events = line.split(\"|\")[2].split(\" \")\n",
    "                        if omit[count]==1 or newsents[count]==[]:\n",
    "                            exclude.append(1)\n",
    "                        else:\n",
    "                            if disease_prev[count] == 1:  \n",
    "                                visits = newsents[count][:target[count]]\n",
    "                                if visits == []:\n",
    "                                    exclude.append(1)\n",
    "                                else:\n",
    "                                    exclude.append(0)\n",
    "                                    feed_events = list(np.concatenate(visits))\n",
    "\n",
    "                                    \n",
    "                                    te = len(feed_events)\n",
    "                                    \n",
    "                                    \n",
    "                                    \n",
    "                            \n",
    "                                    weighted_events = [(e,  math.exp(decay*(j-te+1)/te)) for j, e in enumerate(newfeed) if e in events]\n",
    "                                    sum_weights = sum(weight for event,weight in weighted_events)\n",
    "                                    \n",
    "                                    with tf.Session() as sess:\n",
    "                                        # It is necessary to initialize variables once before running inference.\n",
    "                                        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "                                        # Create batches of data.\n",
    "                                        context_ids = batcher.batch_sentences([feed_events])\n",
    "\n",
    "\n",
    "                                        # Compute ELMo representations (here for the input only, for simplicity).\n",
    "                                        elmo_context_input_ = sess.run(\n",
    "                                            elmo_context_input['weighted_op'],\n",
    "                                            feed_dict={context_character_ids: context_ids}\n",
    "                                        )\n",
    "\n",
    "\n",
    "                                    flat_embeddings = list(itertools.chain(*elmo_context_input_))\n",
    "                                    for a in weighted_events:\n",
    "                                        event, weight = a\n",
    "                                        patient_seq.append(weight*flat_embeddings[feed_events.index(event)])\n",
    "                                    patient_seq_all_disease.append(sum(patient_seq)/sum_weights)\n",
    "\n",
    "                            else:\n",
    "                                visits = newsents[count]  \n",
    "                                if visits == []:\n",
    "                                    exclude.append(1)\n",
    "                                else:\n",
    "                                    feed_events = list(np.concatenate(visits))\n",
    "                                    exclude.append(0)\n",
    "                                    \n",
    "                                    te = len(feed_events)\n",
    "                                    \n",
    "\n",
    "                                    \n",
    "                                    \n",
    "                                    weighted_events = [(e,  math.exp(decay*(j-te+1)/te)) for j, e in enumerate(newfeed) if e in events]\n",
    "                                    sum_weights = sum(weight for event,weight in weighted_events)\n",
    "                                                                        with tf.Session() as sess:\n",
    "                                        # It is necessary to initialize variables once before running inference.\n",
    "                                        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "                                        # Create batches of data.\n",
    "                                        context_ids = batcher.batch_sentences([feed_events])\n",
    "\n",
    "\n",
    "                                        # Compute ELMo representations (here for the input only, for simplicity).\n",
    "                                        elmo_context_input_ = sess.run(\n",
    "                                            elmo_context_input['weighted_op'],\n",
    "                                            feed_dict={context_character_ids: context_ids}\n",
    "                                        )\n",
    "\n",
    "\n",
    "                                    flat_embeddings = list(itertools.chain(*elmo_context_input_))\n",
    "                                    for a in weighted_events:\n",
    "                                        event, weight = a\n",
    "                                        patient_seq.append(weight*flat_embeddings[feed_events.index(event)])\n",
    "                                    patient_seq_all_disease.append(sum(patient_seq)/sum_weights)\n",
    "            return patient_seq_all_disease, exclude\n",
    "\n",
    "\n",
    "\n",
    "        #Update labels for presence of target diagnosis\n",
    "        def update_data(exclude,disease_data,data_disease):                \n",
    "            disease_final = []\n",
    "            for i in range(0,len(exclude)):\n",
    "                if exclude[i] == 0:\n",
    "                    disease_final.append(disease_data[i])\n",
    "\n",
    "            data_disease[passed[q]] = disease_final\n",
    "\n",
    "            X_data_disease=data_disease.iloc[:,0:100]\n",
    "            Y_data_disease =data_disease.iloc[:,100]\n",
    "\n",
    "\n",
    "            return X_data_disease, Y_data_disease\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "        patient_seq_all_disease, exclude= patient_vec(train_files, newsents,  disease_prev,target_loc,omit1)\n",
    "\n",
    "        data_disease=pd.DataFrame(patient_seq_all_disease)\n",
    "\n",
    "        X_train_disease, Y_train_disease = update_data(exclude,disease_train,data_disease)\n",
    "\n",
    "\n",
    "        sentences, omit1, disease_prev = prep_data(valid_files)\n",
    "\n",
    "        newsents = split_sentences(sentences)\n",
    "\n",
    "        newsents2 = split_sentences2(sentences)\n",
    "\n",
    "        target_loc = adjust_all(newsents2)\n",
    "\n",
    "        disease_test, omit1 = adjust_exclusions(sentences, newsents,omit1)\n",
    "              \n",
    "        patient_seq_all_disease_test, exclude = patient_vec(valid_files, newsents,disease_prev, target_loc,omit1)\n",
    "      \n",
    "        data_disease_test = pd.DataFrame(patient_seq_all_disease_test)\n",
    "\n",
    "              \n",
    "        X_disease_test, Y_disease_test = update_data(exclude,disease_test,data_disease_test)\n",
    "\n",
    "        if pred =='lasso':\n",
    "\n",
    "            searchCV_lasso = LogisticRegressionCV(Cs=list(np.power(10.0, np.arange(-10, 10))),penalty='l1',\n",
    "            cv=5,fit_intercept=True,solver='liblinear',max_iter = 10000,scoring=\"f1\")\n",
    "            lasso_fit_disease = searchCV_lasso.fit(X_train_disease,Y_train_disease)\n",
    "            lasso_prob_disease = lasso_fit_disease.predict_proba(X_train_disease)[:,1]\n",
    "\n",
    "            lasso_prob_disease = lasso_fit_disease.predict_proba(X_disease_test)[:,1]\n",
    "            auc_disease_lasso = metrics.roc_auc_score(Y_disease_test, lasso_prob_disease)\n",
    "            AUC_hold.append(auc_disease_lasso)\n",
    "            F1_hold.append(f1_score(Y_disease_test, lasso_fit_disease.predict(X_disease_test)))\n",
    "\n",
    "\n",
    "        if pred == 'DL':\n",
    "\n",
    "            print(\"Reading files...\"),\n",
    "            tr_dataframe = data_disease\n",
    "            te_dataframe = data_disease_test\n",
    "\n",
    "            \n",
    "            \n",
    "\n",
    "            print(\"Fineshed\\n\")\n",
    "\n",
    "            print(\"Compressing data...\"),\n",
    "            tr_X = tr_dataframe.iloc[:, :-1].values\n",
    "            tr_Y = tr_dataframe.iloc[:, -1].values\n",
    "            te_X = te_dataframe.iloc[:, :-1].values\n",
    "            te_Y = te_dataframe.iloc[:, -1].values\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "            # model_DL = KerasRegressor(build_fn = baseline_model(units = Grid.best_params_['units'], act = Grid.best_params_['act']), nb_epoch=2000, batch_size=300,verbose = 0)\n",
    "\n",
    "            model_DL = Sequential()\n",
    "            model_DL.add(Dense(55,activation = 'relu'))\n",
    "            model_DL.add(Dense(20,activation='relu'))\n",
    "            model_DL.add(Dense(1,activation = 'sigmoid'))\n",
    "            model_DL.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "\n",
    "\n",
    "            # In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # In[32]:\n",
    "\n",
    "\n",
    "            model_DL.fit(tr_X,tr_Y,batch_size = 100, epochs=70, verbose = 0)\n",
    "            tr_predict_Y = model_DL.predict(tr_X)\n",
    "            te_predict_Y = model_DL.predict(te_X)    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # ROC AUC\n",
    "            auc = metrics.roc_auc_score(te_Y, te_predict_Y)\n",
    "            print('ROC AUC: %f' % auc)\n",
    "            yhat_classes = model_DL.predict_classes(te_X, verbose=0)\n",
    "            f1 = f1_score(te_Y, yhat_classes)\n",
    "            print('F1: %f' % f1)\n",
    "\n",
    "            AUC_hold.append(auc)\n",
    "            F1_hold.append(f1)\n",
    "\n",
    "    AUC.append(np.mean(AUC_hold))\n",
    "    F1.append(np.mean(F1_hold))\n",
    "   \n",
    "\n",
    "\n",
    "from pandas import DataFrame\n",
    "df_auc = DataFrame(AUC)\n",
    "df_auc.to_csv(path_auc)\n",
    "df_f1 = DataFrame(F1)\n",
    "df_f1.to_csv(path_F1)\n",
    "\n",
    "\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#elmo_data = pd.DataFrame([passed, AUC_logistic_elmo, AUC_lasso_elmo,AUC_DL_elmo,F1_logistic_elmo,F1_lasso_elmo,F1_DL_elmo, prevalence], index = [\"Disease\", \"AUC Logistic\",\"AUC lasso\", \n",
    "                                                                                                             \"AUC DL\",\"F1 Logistic\",\"F1 lasso\",\"F1 DL\",\"Prevalence\"])\n",
    "#elmo_data\n",
    "#elmo_data.to_excel('/home/egetzen/elmo_data.xlsx') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
