{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use with python/3.6.0 and tensorflow/1.10\n",
    "\n",
    "import math\n",
    "import sys\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import random\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn import metrics\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from glove import Corpus, Glove\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.spatial.distance as ds\n",
    "from bilm import Batcher, BidirectionalLanguageModel, weight_layers\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, Dropout\n",
    "from tensorflow.python.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.python.keras.constraints import maxnorm\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import make_scorer, r2_score, mean_absolute_error, mean_squared_error\n",
    "import itertools\n",
    "\n",
    "from bert_serving.client import BertClient\n",
    "bc = BertClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data, define target diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## change data path to where the mimic sequences are located\n",
    "data_path = \"/users/emily/Documents/mimic_seq/\"\n",
    "random.seed(1)\n",
    "\n",
    "def get_file_list(data_path):\n",
    "    # list for files\n",
    "    train_files = []\n",
    "    valid_files = []\n",
    "    full_data_files = []\n",
    "\n",
    "    for i in range(7):\n",
    "        train_files.append(data_path + 'test_'+str(i))\n",
    "    for i in range(7,10):\n",
    "        valid_files.append(data_path + 'test_'+str(i))\n",
    "    for i in range(10):\n",
    "        full_data_files.append(data_path + 'test_'+str(i))\n",
    "    \n",
    "    return train_files, valid_files, full_data_files\n",
    "\n",
    "## define target diagnosis\n",
    "\n",
    "#test = ['d_041','d_250','d_401','d_414','d_424','d_427','d_530','d_585']\n",
    "test = ['d_585']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files, valid_files, full_data_files = get_file_list(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate embeddings dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/[py3.6]/lib/python3.6/site-packages/bert_serving/client/__init__.py:290: UserWarning: server does not put a restriction on \"max_seq_len\", it will determine \"max_seq_len\" dynamically according to the sequences in the batch. you can restrict the sequence length on the client side for better efficiency\n",
      "  warnings.warn('server does not put a restriction on \"max_seq_len\", '\n"
     ]
    }
   ],
   "source": [
    "for q in range(len(test)):\n",
    "    \n",
    "    ### Take raw mimic sequence data and generate a list of medical concepts for each patient\n",
    "    \n",
    "    def create_disease_sentences(files):\n",
    "        count = -1\n",
    "        sentences = []\n",
    "        demographic = []\n",
    "        final_dx = []\n",
    "        disease_prev = []\n",
    "\n",
    "        for i in files:\n",
    "            with open(i) as f:\n",
    "                for s in f:\n",
    "                    count = count + 1\n",
    "                    demographic.append(s.split(\"|\")[1].split(\" \"))\n",
    "                    sentences.append(s.split(\"|\")[2].split(\" \") +\n",
    "                                     s.split(\"|\")[3].replace(\"\\n\", \"\").split(\" \"))\n",
    "                    next_diags = s.split(\"|\")[0].split(\",\")\n",
    "                    final_dx.append(next_diags)\n",
    "                    prev_diags = [e for e in s.split(\"|\")[2].split(\" \") if e.startswith(\"d_\")]\n",
    "\n",
    "                    if test[q] in prev_diags: # if disease exit in the previous diagnosis\n",
    "     \n",
    "                            disease_prev.append(1)\n",
    "                    \n",
    "                    else:\n",
    "                        disease_prev.append(0)\n",
    "\n",
    "        ## create a vector indicating which patients have disease as a final diagnosis\n",
    "        disease_final_ori = []\n",
    "        for sublist in final_dx:\n",
    "            if test[q] in sublist:\n",
    "                disease_final_ori.append(1)\n",
    "            else:\n",
    "                disease_final_ori.append(0)\n",
    "\n",
    "\n",
    "        return sentences, demographic, final_dx, disease_prev, disease_final_ori\n",
    "    sentences, demographic, final_dx_train, disease_prev, disease_train_ori = create_disease_sentences(train_files)\n",
    "    \n",
    "    ### Identify unique medical concepts in the data\n",
    "    def generate_bert_dict(sentences):\n",
    "\n",
    "        flat_sentences = list(itertools.chain(*sentences))\n",
    "\n",
    "        events = np.unique(flat_sentences)\n",
    "        events = np.ndarray.tolist(events)\n",
    "\n",
    "        bert_dict = []\n",
    "        bertdict = []\n",
    "\n",
    "        return events, bert_dict, bertdict\n",
    "    events, bert_dict, bertdict = generate_bert_dict(sentences)\n",
    "\n",
    "    ## Get the vectors corresponding to the entire patient history\n",
    "    \n",
    "    def generate_patient_disease_vectors(files, events, bert_dict):\n",
    "        count = -1\n",
    "        count2 = 0\n",
    "\n",
    "        patient_seq_all_disease = []\n",
    "        patient_seq_all_disease_1 = []\n",
    "        patient_sen_disease = []\n",
    "        exclude = []\n",
    "\n",
    "        _, _, _, disease_prev, disease_final_ori = create_disease_sentences(files)\n",
    "\n",
    "        for i in files: \n",
    "            with open(i) as f:\n",
    "                for line in f:\n",
    "                    count = count + 1\n",
    "                    patient_seq = []\n",
    "                    feed_events = line.split(\"|\")[2].split(\" \")\n",
    "\n",
    "                    if disease_prev[count] == 1: # Only include events before the target diagnosis\n",
    "                        target = feed_events.index(test[q])\n",
    "                        feed_events = feed_events[:target]\n",
    "                        if feed_events == []:\n",
    "                            count2 = count2+1\n",
    "                            exclude.append(1) #exclude patients that have no medical concepts before the target diagnosis\n",
    "                        else:    \n",
    "                            exclude.append(0)\n",
    "                            te = len(feed_events)\n",
    "                            \n",
    "                            patient_seq = 0\n",
    "                            patient_seq_all_disease.append(patient_seq)\n",
    "                            newfeed = []\n",
    "                            newfeed1 = []\n",
    "                            newfeed2 = []\n",
    "                            patient_seq = []\n",
    "                            for n in range(len(feed_events)):\n",
    "                                newfeed.append(feed_events[n].replace(\"_\",r\"\"))\n",
    "                            for y in range(len(newfeed)):\n",
    "                                newfeed1.append(newfeed[y].replace(\".\",r\"\"))\n",
    "                            for z in range(len(newfeed1)):\n",
    "                                newfeed2.append(newfeed1[z].replace(\"/\",r\"q\"))\n",
    "                            feed_events = newfeed2\n",
    "\n",
    "                            feed = ' '.join(feed_events)\n",
    "                            patient_seq = bc.encode([feed]) #Generate BERT sentence embedding for all events in history\n",
    "                            patient_seq_all_disease_1.append(patient_seq)\n",
    "\n",
    "                            \n",
    "                            \n",
    "\n",
    "                    else: \n",
    "                        exclude.append(0)\n",
    "                        te = len(feed_events)\n",
    "                        \n",
    "                        patient_seq = 0\n",
    "                        patient_seq_all_disease.append(patient_seq)\n",
    "                        newfeed = []\n",
    "                        newfeed1 = []\n",
    "                        newfeed2 = []\n",
    "                        patient_seq = []\n",
    "                        for n in range(len(feed_events)):\n",
    "                            newfeed.append(feed_events[n].replace(\"_\",r\"\"))\n",
    "                        for y in range(len(newfeed)):\n",
    "                            newfeed1.append(newfeed[y].replace(\".\",r\"\"))\n",
    "                        for z in range(len(newfeed1)):\n",
    "                            newfeed2.append(newfeed1[z].replace(\"/\",r\"q\"))\n",
    "                        feed_events = newfeed2\n",
    "\n",
    "                        feed = ' '.join(feed_events)\n",
    "                        patient_seq = bc.encode([feed]) #Generate BERT sentence embedding for all events in history\n",
    "                        patient_seq_all_disease_1.append(patient_seq)\n",
    "\n",
    "        patient_seq_all_disease = []\n",
    "        \n",
    "    \n",
    "        flat_embeddings = list(itertools.chain(*patient_seq_all_disease_1))\n",
    "\n",
    "        ## Create a dataframe for all the patient vectors\n",
    "        data_disease_1=pd.DataFrame(flat_embeddings)\n",
    "\n",
    "        ## Create the train labels, make sure to get rid of labels where we exclude patients\n",
    "        disease_final = []\n",
    "        for i in range(0,len(exclude)):\n",
    "            if exclude[i] == 0:\n",
    "                disease_final.append(disease_final_ori[i])\n",
    "\n",
    "\n",
    "        data_disease_1[test[q]] = disease_final\n",
    "\n",
    "\n",
    "        X_disease_1=data_disease_1.iloc[:,0:100]\n",
    "        Y_disease = data_disease_1.iloc[:,100]\n",
    "\n",
    "        return X_disease_1, Y_disease, data_disease_1\n",
    "\n",
    "    X_train_disease_1, Y_train_disease, data_disease_1 = generate_patient_disease_vectors(train_files, events, bert_dict)\n",
    "    X_disease_test_1, Y_disease_test, data_disease_test_1 = generate_patient_disease_vectors(valid_files, events, bert_dict)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save dataframe in xlsx file\n",
    "\n",
    "import openpyxl\n",
    "import pandas as pd\n",
    "data_disease_1.to_excel('/users/emily/Documents/sentence/ckd_train.xlsx')\n",
    "data_disease_test_1.to_excel('/users/emily/Documents/sentence/ckd_test.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
